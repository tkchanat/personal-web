(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-6c28d888"],{"02bf":function(e,t,n){var r={"./curve-editor.md":"a60b","./cvc.md":"0c73","./dithering-transparency.md":"2359","./game-engine.md":"3efb","./kradia-the-little-tales.md":"b6a4","./laplacian-editing.md":"c900","./marching-cubes.md":"61e8","./raindrop-shader.md":"17a2","./test.md":"89c5","./unsharp-masking.md":"ffd2","./volumetric-cloud.md":"e76c","./warfare-dimension.md":"ec32","./wave-sumo.md":"02bf9","./woods-of-kodama.md":"77c8"};function i(e){var t=a(e);return n(t)}function a(e){if(!n.o(r,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return r[e]}i.keys=function(){return Object.keys(r)},i.resolve=a,e.exports=i,i.id="02bf"},"02bf9":function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nA two-player sumo combat game made in Global Game Jam 2017. The whole game was produced under 48 hours with a team of 3.\r\n\r\nThis is a two-player sumo game. The goal is to use wave and power-up to push your opponent out of the battle ground in order to gain points. The player who first score 5 points wins.\r\n\r\n## About This Project\r\n- Programming Language: C#\r\n- Game Engine: Unity3D\r\n- Development Environment: Visual Studio, Clip Studio Paint\r\n- Team Size: 3\r\n- Role: Lead Gameplay Programmer, 2D Artist\r\n\r\n## Responsibilities\r\n- Design and implement gameplay mechanics\r\n- Background and UI design,\r\n- Conduct project flow\r\n\r\n## Useful Links\r\n- :link: [Demo Video](https://youtu.be/KGE5js42aFQ)\r\n- :link: [Global Game Jam Site](https://globalgamejam.org/2017/games/wave-sumo)"},"0c73":function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nThis is a two-player VR simulation serious game developed during my summer internship at Queen Elizabeth Hospital. The purpose is to aid medical trainees to familiarize with Central Venous Catheters(CVC) procedures as to avoid incidents. In the simulation, one player takes the nurse role while another acts as a doctor. The nurse is responsible to prepare equipment required for performing the CVC and run through the checklist items. The actual procedures are then performed by the doctor. Task boards are placed on each side of the room so that players can learn while enjoying the experience.\r\n\r\n## About This Project\r\n- Programming Language: C#\r\n- Development Environment: Unity3D, Visual Studio, 3DSMax\r\n- Hardware: HTC Vive Pro, Leap Motion\r\n- Team Size: 2\r\n- Role: Programmer\r\n\r\n## Responsibilities\r\n- Conduct project flow\r\n- Implement main gameplay mechanics\r\n- Enhance user experience according to professional feedbacks\r\n- Implement multiplayer (up to 2 players) over LAN network using Photon Unity Networking 2 (PUN2) library\r\n\r\n## Useful Links\r\n- :link: [Internship Department - Multi-Disciplinary Simulation and Skills Center, Queen Elizabeth Hospital](https://www3.ha.org.hk/qeh/department/mdssc/en_index.html)"},"16e1":function(e,t,n){},"17a2":function(e,t,n){"use strict";n.r(t),t["default"]="## Motivation\r\nOne day I've been wandering on ShaderToy, and I came across a piece of work named [Heartfelt](https://www.shadertoy.com/view/tlVGWK). At first glance I knew I'm going to breakdown the code and learn something from it because this work really captures the essence of rain. But then I found out the name of **Martijin Steinrucken** (a.k.a. [The Art of Code](https://www.youtube.com/c/TheArtofCodeIsCool) from YouTube) was credited within the code. So I checked his YouTube channel and found a [tutorial](https://www.youtube.com/watch?v=EBrAdahFtuo) explaining in detail of how the shader works. And here we are, trying to create one myself by following his video. :umbrella:\r\n\r\n---\r\n\r\n## Development\r\nI won't cover much here since Martijin is an awesome guy and you should check out his video if you want an in-depth explanation on how he models the raindrops. But I'll still briefly show you the idea behind the procedural generated raindrops using only mathematics tricks. \r\n\r\n### Dynamic Drops\r\n| Moving grids /w perfectly-timed vertical movements | Shape distortion & horizontal movements | Trails /w UV coordinates | Apply as UV offsets |\r\n| :------------: | :------------: | :------------: | :------------: |\r\n| ![](/images/rd1.gif) | ![](/images/rd2.gif) | ![](/images/rd3.gif) | ![](/images/rd4.gif) |\r\n\r\n### Static Drops\r\n| Tiled UV using sin(x) /w noise distortion | Smoothstep(x) on the sum of RG channels | Smoothstep(x) on layered Perlin noise | Apply as mask |\r\n| :------------: | :------------: | :------------: | :------------: |\r\n| ![](/images/rd5.png) | ![](/images/rd6.png) | ![](/images/rd7.gif) | ![](/images/rd8.gif) |\r\n\r\n### Composition\r\nLayering two dynamic drops and one static drops and apply them as the UV offsets for sampling textures (either backgrounds or frame buffers) can compose the final effect as below.\r\n![](/images/rd9.gif)\r\nFor a final touch, I've added a [vignette effect](https://www.shadertoy.com/view/lsKSWR) for the rainy day vibe. I even hand-picked this photo that I've taken on a rainy day at Melbourne city. :foggy:\r\n\r\n## Exploration\r\nWe all knew straight up following others code wasn't cool, so I went experimenting some more variations with this shader. At least I don't like the idea of blatantly copying others work without contributing!\r\n\r\n### Slope-aware Raindrops\r\nI was thinking of apply this shader onto any kind of surfaces, both concave and convex, and the raindrops should slide off according to the surface slope. However after some digging, a guy named **Taizyd Korambayil** has done it before and of course he wrote [a blog](https://deepspacebanana.github.io/deepspacebanana.github.io/blog/shader/art/unreal%20engine/Rainy-Surface-Shader-Part-1) about it. Now that's cool, but he's using the `triplanar projection` method to achieve it. And I wonder if it's possible to render raindrops on surface procedurally.  \r\n\r\n#### 1st Attempt - Rotated UV Coordinates\r\nSince the shader above is based on the UV of the geometry, initially I was thinking a correct UV orientation would do the trick (i.e. if a cube is tilted 45 degree, the UV coordinates should be rotated accordingly). Needless to say, that's too naive to think it that way because obviously different geometries/meshes have **different UV mapping and topology**. So this approach immediately puts on a halt.\r\n\r\n#### 2nd Attempt - World Position\r\nJust for record, I once thought of using the fraction part of a scaled world coordinates as the reference grid for the raindrops. But we're not dealing with volumetric data, instead we need the surface information rather than its global position. Furthermore, using the XYZ-axis as the basis has no difference with the `triplanar projection` method, where we still get the weird overlapping artifacts around the non-axis alignign corners. Thus I concluded that's a no-go for me.\r\n\r\n#### 3rd Attempt - Surface Tangents\r\nHow about utilizing the surface tangents you ask? We could calculate the downward vector on a given surface by some vector projection tricks using the global up vector, similar to the one used in the `Gram-Schmidt process`. That's the closest one I believe that might work! ...unless it doesn't :cry:\r\n| Cube | Torus |\r\n| :------------: | :------------: |\r\n| ![](/images/rd10.gif) | ![](/images/rd11.gif) |\r\n\r\nYou see, the result on a cube is very promising. But for the torus...ughhhhh, it's not looking right. I have no idea why it won't work on concave surfaces. So I basically gave up at this point. Perhaps I'm too ambitious at the first place. But hey, I had fun learning something from it! :thumbsup:\r\n\r\n---\r\n\r\n## Tool Used\r\n- Programming Language: HLSL\r\n- Environment: [SHADERed](https://github.com/dfranx/SHADERed)"},2359:function(e,t,n){"use strict";n.r(t),t["default"]="## Motivation\r\n![](/images/dither_motiv.png)\r\nDon't you just hate when the camera clipping through geometries when you're in a free camera or an arcball camera? Being able to look through objects can sometimes take away the immersive experience of the viewer. Especially in games, players will identify something was wrong or even thinking they're \"glitching\" the game. But what if your camera is intented to go through objects? Are there methods to smoothly transition from opaque surfaces to the background?\r\n\r\n### Alpha Blending\r\nSome would say enabling alpha testing supported by the rendering API could potentially work. The idea is to lower the surface alpha value based on the distance to the viewing camera. Then it may appears to be fading from the approaching camera. \r\n![](/images/painters.jpg)\r\nThat's true until you are working with overlapping geometries. And according to the `Painter's Algorithm`, which is a way to sort all transparent objects and paint them individually from back to front, it's impossible to identify which part of the geometry is being overlapped or not (demonstrated by the above image). Hence becoming difficult to render them properly without being bothered by the artifacts. So, we need a smarter way to blend multiple opaque objects together (at least visually).\r\n\r\n---\r\n\r\n## Development\r\n| Original | Ordered | Error-Diffusion |\r\n| :------: | :-----: | :-------------: |\r\n| ![](/images/dither_orig.png) | ![](/images/dither_order.png) | ![](/images/dither_error.png) |\r\n![](/images/bayer_matrix.png)\r\n\r\n`Dithering`, a method often used by graphics engineerers to trick your eyes. By interlacing the pixels in a specific pattern, your brain will basically treat them as a normal gradient. The above image is using an `ordered dithering` algorithm using a `Bayer matrix`. Of course there're different kind of algorithms, like `error-diffusion dithering` which gives an even smoother transitions between color values. But for the sake of performance, `ordered dithering` has the least computation complexity. \r\n\r\n### Idea\r\nFirst we have to convert the screen space to pixel coordinates by multiplying it by the viewport resolution. Then using a modulo operator on both x and y axis to slice up the canvas. For each `4x4` pixel area, apply the `Bayer matrix` to calculate the threshold value for that pixel position. Here's what the matrix looks like:\r\n$\\begin{bmatrix}\r\n  0  & 8  & 2  & 10 \\\\\r\n  12 & 4  & 14 & 6  \\\\\r\n  3  & 11 & 1  & 9  \\\\\r\n  15 & 7  & 13 & 5\r\n\\end{bmatrix}$\r\n\r\nFor this implementation, we will be incrementing all elements by 1 and divide them by 17. Since we would like to have 17 shades of gray (i.e. all black&white + 15 aternating dot patterns), the division here is very crucial. After that, we will `discard` all the pixels that are having an alpha value lower than its corresponding threshold value. \r\n\r\n#### Shader Graph\r\n![](/images/dither_graph.png)\r\nFinally, hooking up the alpha value by the distance from the surface's world position to the camera. Setting a fade distance parameter can limit the activition range of the dithering effect. \r\n\r\nAnd there you have it! I'm thinking of combining this effect with the Schrödinger's cat:cat2: would be a cool demonstration. Now a conscious being can finally observe a quantum object without clipping through reality! :thumbsup::thumbsup:\r\n\r\n### Result\r\n![](/images/dither.gif)"},"3efb":function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nYet another cross-platform OpenGL game engine written in C++. The goal is to push our limits and dig deeper into modern game engine design and architecture.\r\n\r\n## About This Project\r\n- Programming Language: C++\r\n- Development Environment: Visual Studio, Xcode\r\n- Team Size: 4\r\n- Role: Lead Programmer\r\n\r\n## Features\r\n- Graphics\r\n  - Physically-based rendering workflow\r\n  - Image-based lighting\r\n  - Shadow mapping\r\n  - Deferred rendering pipeline\r\n  - Sprite and text rendering\r\n- Components\r\n  - Skeletal animation and crossfade blending\r\n  - GPU particle system\r\n  - 3D audio system\r\n  - Collision detection on bounding volumes\r\n  - AI pathfinding algorithms\r\n- Engine\r\n  - Binary serialization on game assets\r\n  - Worker thread pool\r\n  - Lua as scripting language\r\n\r\n## Responsibilities\r\n- Design and implement a cross-platform OpenGL game engine using C++\r\n- Conduct project flow and distribute tasks among team members\r\n\r\n## Useful Links\r\n- :link: [GitHub Repository](https://github.com/tkchanat/LobsterGameEngine)"},"61e8":function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nA course project for Spring 2019 [Computer Graphics and Interaction (DH2323)](https://www.kth.se/student/kurser/kurs/DH2323?l=en) in Royal Institute of Technology. This work presents a real-time smoothed particle hydrodynamics visualization with metaball.\r\nThis is a two-person group work to show our knowledge over OpenGL rendering technique, as well as real-time physics simulation and optimization. We used smoothed particle hydrodynamics for the particle physics evaluated on the CPU. While the rendering is done mainly on geometry shader. Marching cube algorithm is implemented for visualizing the isosurface of metaballs. The surface is then shaded using phong light model. You can find more about our process in our [development blog](https://alexander-hjelm.github.io/metaballs-glfw/making-voxel-cubes/).\r\n:computer: [GitHub Repository](https://github.com/Alexander-Hjelm/metaballs-glfw)\r\n:page_with_curl: [Project Proposal](https://github.com/Alexander-Hjelm/metaballs-glfw/wiki/Project-Proposal-v2)\r\n\r\n---\r\n\r\n## Tool Used\r\n- Programming Language: C++, GLSL\r\n- Library Used: [GLFW](https://www.glfw.org/)\r\n- Environment: Xcode, Vim"},"63b7":function(e,t,n){},"77c8":function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nThe world has lost most of it's colour and one of the last remaining great trees gives birth to you to restore colour to the world. You are a Groot brother Sopr and you need to bring back colors to world by gathering fruits (peach, grapes and bananas) and throwing them into sad animals (flamingo, frog, chicken).\r\n\r\n## About This Project\r\n- Game Engine: Unity3D\r\n- Development Environment: Piskil\r\n- Team Size: 8\r\n- Role: 2D Artist/Animator\r\n\r\n## Responsibility\r\n- Animate all props and character sprites in the game\r\n\r\n## Useful Links\r\n- :link: [Global Game Jam Site](https://globalgamejam.org/2019/games/woods-kodama)\r\n- :link: [GitHub Repository](https://github.com/staffannase/global-game-jam)\r\n- :link: [Download Link](https://drive.google.com/file/d/1p9YkM9ctQEnLkwyEfWeckeqM1vRsaN2N/view?usp=sharing)"},"89c5":function(e,t,n){"use strict";n.r(t),t["default"]='# h1 Heading 8-)\r\n<h2> h2 Heading by HTML</h2>\r\n## h2 Heading\r\n### h3 Heading\r\n\r\n## Horizontal Rules\r\n\r\n___\r\n\r\n---\r\n\r\n***\r\n\r\n## Typographic replacements\r\n\r\nEnable typographer option to see result.\r\n\r\n(c) (C) (r) (R) (tm) (TM) (p) (P) +-\r\n\r\ntest.. test... test..... test?..... test!....\r\n\r\n!!!!!! ???? ,,  -- ---\r\n\r\n"Smartypants, double quotes" and \'single quotes\'\r\n\r\n\r\n## Emphasis\r\n\r\n**This is bold text**\r\n\r\n__This is bold text__\r\n\r\n*This is italic text*\r\n\r\n_This is italic text_\r\n\r\n~~Strikethrough~~\r\n\r\n\r\n## Blockquotes\r\n\r\n\r\n> Blockquotes can also be nested...\r\n>> ...by using additional greater-than signs right next to each other...\r\n> > > ...or with spaces between arrows.\r\n\r\n\r\n## Lists\r\n\r\nUnordered\r\n\r\n+ Create a list by starting a line with `+`, `-`, or `*`\r\n+ Sub-lists are made by indenting 2 spaces:\r\n  - Marker character change forces new list start:\r\n    * Ac tristique libero volutpat at\r\n    + Facilisis in pretium nisl aliquet\r\n    - Nulla volutpat aliquam velit\r\n+ Very easy!\r\n\r\nOrdered\r\n\r\n1. Lorem ipsum dolor sit amet\r\n2. Consectetur adipiscing elit\r\n3. Integer molestie lorem at massa\r\n\r\n\r\n1. You can use sequential numbers...\r\n1. ...or keep all the numbers as `1.`\r\n\r\nStart numbering with offset:\r\n\r\n57. foo\r\n1. bar\r\n\r\n\r\n## Code\r\n\r\nInline `code`\r\n\r\nIndented code\r\n\r\n    // Some comments\r\n    line 1 of code\r\n    line 2 of code\r\n    line 3 of code\r\n\r\n\r\nBlock code "fences"\r\n\r\n```\r\nSample text here...\r\n```\r\nSyntax highlighting\r\n\r\n``` javascript\r\nvar foo = function (bar) {\r\n  return bar++;\r\n};\r\n\r\nconsole.log(foo(5));\r\n```\r\n\r\n``` go\r\npackage main\r\n\r\nimport "fmt"\r\n\r\nfunc main() {\r\n\tfmt.Println("Hello, world!")\r\n}\r\n```\r\n\r\n## Tables\r\n\r\n| Option | Description |\r\n| ------ | ----------- |\r\n| data   | path to data files to supply the data that will be passed into templates. |\r\n| engine | engine to be used for processing templates. Handlebars is the default. |\r\n| ext    | extension to be used for dest files. |\r\n\r\nRight aligned columns\r\n\r\n| Option | Description |\r\n| ------:| -----------:|\r\n| data   | path to data files to supply the data that will be passed into templates. |\r\n| engine | engine to be used for processing templates. Handlebars is the default. |\r\n| ext    | extension to be used for dest files. |\r\n\r\n## Links\r\n\r\n[vue-markdown](https://github.com/miaolz123/vue-markdown)\r\n\r\n[link with title](https://github.com/miaolz123/vue-markdown "VueMarkdown")\r\n\r\nAutoconverted link https://github.com/miaolz123/vue-markdown (enable linkify to see)\r\n\r\n\r\n## Images\r\n\r\n![Minion](dist/img/minion.png)\r\n\r\nLike links, Images also have a footnote style syntax\r\n\r\n![Alt text][id]\r\n\r\nWith a reference later in the document defining the URL location:\r\n\r\n[id]: dist/img/minion.png  "The Dojocat"\r\n\r\n\r\n### Emojies\r\n\r\n> Classic markup: :wink: :cry: :laughing: :yum:\r\n>\r\n> Shortcuts (emoticons): :-) :-( 8-) ;)\r\n\r\n\r\n### Subscript / Superscript\r\n\r\n- 19^th^\r\n- H~2~O\r\n\r\n\r\n### \\<ins>\r\n\r\n++Inserted text++\r\n\r\n\r\n### \\<mark>\r\n\r\n==Marked text==\r\n\r\n\r\n### Footnotes\r\n\r\nFootnote 1 link[^first].\r\n\r\nFootnote 2 link[^second].\r\n\r\nInline footnote^[Text of inline footnote] definition.\r\n\r\nDuplicated footnote reference[^second].\r\n\r\n[^first]: Footnote **can have markup**\r\n\r\n    and multiple paragraphs.\r\n\r\n[^second]: Footnote text.\r\n\r\n\r\n### Definition lists\r\n\r\nTerm 1\r\n\r\n:   Definition 1\r\nwith lazy continuation.\r\n\r\nTerm 2 with *inline markup*\r\n\r\n:   Definition 2\r\n\r\n        { some code, part of Definition 2 }\r\n\r\n    Third paragraph of definition 2.\r\n\r\n_Compact style:_\r\n\r\nTerm 1\r\n  ~ Definition 1\r\n\r\nTerm 2\r\n  ~ Definition 2a\r\n  ~ Definition 2b\r\n\r\n\r\n### Abbreviations\r\n\r\nThis is HTML abbreviation example.\r\n\r\nIt converts "HTML", but keep intact partial entries like "xxxHTMLyyy" and so on.\r\n\r\n*[HTML]: Hyper Text Markup Language'},a60b:function(e,t,n){"use strict";n.r(t),t["default"]="## Motivation\r\nThis is a solo project which I made in my spare time. This is the first time I get into real computer graphics programming with OpenGL and no external library except GLFW for window creation. The application is basically a playground where you can explore the behaviour of Bezier and B-spline curves.\r\n\r\n---\r\n\r\n## Description\r\nUser can add control points by right clicking any empty space, and able to drag those control points freely within the window. Pressing 1 can show/hide the lines defining the convex hull, 2 for toggling between Bezier and B-spline curves, 3 to form a loop with all control points. The resolution (step size) can be increased or decreased with +/- button.\r\n\r\n---\r\n\r\n## Tool Used\r\n- Programming Language: C++, GLSL\r\n- Library Used: [GLFW](https://www.glfw.org/)\r\n- Environment: Xcode"},b62c:function(e,t,n){"use strict";var r=n("16e1"),i=n.n(r);i.a},b6a4:function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nAn indie combat game developed with a small group of design professionals. Kradia: The Little Tales is a demo single-player action game, an extension of LinerNote HK's Kradia series. The difficulty of the game increases over time, and player needs to clear all as much enemies as possible before within one minute. The goal is to race for the highest score/ranking.\r\n\r\n## About This Project\r\n- Programming Language: C#\r\n- Game Engine: Unity3D\r\n- Development Environment: Visual Studio\r\n- Team Size: 3\r\n- Role: Lead Gameplay Programmer\r\n\r\n## Responsibilities\r\n- Implement player control and combat system\r\n- Balance and improve player experience\r\n\r\n## Useful Links\r\n- :link: [Demo Video](https://www.facebook.com/559289660888279/videos/766324540184789)\r\n- :link: [Download Link](https://goo.gl/X4b1x0)"},c900:function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nThis is a programming work for a post-graduate course - [COMP5411 Advanced Computer Graphics](https://www.cse.ust.hk/pg/courses/#COMP5411). The application stores geometries using the `half edge data structure`. With the help of such data structure, both Laplacian smoothing and naïve Laplacian deformation/editing can be applied in almost real-time.\r\n\r\n---\r\n\r\n#### Laplacian Smoothing\r\n|    Original     |  5th Iteration   |  20th Iteration  |\r\n| :-------------: | :--------------: | :--------------: |\r\n| ![](/images/acg_ls.png) | ![](/images/acg_ls1.png) | ![](/images/acg_ls2.png) |\r\nFor Laplacian smoothing, both `uniform weighting` and `cotangent weighting` are implemented to compare and observe the effects in vertex drifting. See more in this [lecture slide](http://graphics.stanford.edu/courses/cs468-12-spring/LectureSlides/06_smoothing.pdf) from Stanford University.\r\n\r\n---\r\n\r\n#### Laplacian Deformation\r\n|    Original     | 1st Deformation  | 2nd Deformation  |\r\n| :-------------: | :--------------: | :--------------: |\r\n| ![](/images/acg_le.png) | ![](/images/acg_le1.png) | ![](/images/acg_le2.png) |\r\nFor Laplacian deformation, control points are manually selected (in purple) for constructing a `precomputed weight matrix`. Then only the position of constraints are updated in the editing phase. Noted that this naïve approach only account for vertex position but not orientation. This is an implementation of the paper [Laplacian Surface Editing](https://igl.ethz.ch/projects/Laplacian-mesh-processing/Laplacian-mesh-editing/index.php) published by Interactive Geometry Lab.\r\n\r\n---\r\n\r\n## Tool Used\r\n- Programming Language: C++\r\n- Environment: Xcode"},cbaf:function(e,t,n){"use strict";n.r(t);var r=function(){var e=this,t=e.$createElement,r=e._self._c||t;return r("v-container",[r("v-btn",{staticClass:"mb-4",attrs:{color:"primary",text:"",rounded:""},on:{click:function(t){return e.go_back()}}},[r("v-icon",{attrs:{left:""}},[e._v("fa-chevron-left")]),e._v("Back ")],1),r("v-card",{staticClass:"mx-auto",attrs:{"max-width":"1000"}},[r("v-list-item",[r("v-list-item-title",[r("v-icon",[e._v("fa-calendar-alt")]),r("span",{staticClass:"mx-3"},[e._v(e._s(e.project.date))])],1)],1),1==e.project.gallery.length?r("v-img",{staticStyle:{"max-height":"500px"},attrs:{src:"/images/"+e.project.gallery[0]}}):e.project.gallery.length>1?r("v-carousel",{attrs:{continuous:"",cycle:""}},e._l(e.project.gallery,(function(e,t){return r("v-carousel-item",{key:t,attrs:{src:"/images/"+e}})})),1):e._e(),r("v-list-item",[r("v-list-item-content",[r("div",{staticClass:"text-h4"},[e._v(e._s(e.project.title))])])],1),r("vue-markdown",{staticClass:"vue-markdown pa-5",on:{rendered:e.modifyTables},model:{value:e.md,callback:function(t){e.md=t},expression:"md"}},[e._v(e._s(n("02bf")("./"+e.project.markdown).default))])],1)],1)},i=[],a=(n("7db0"),n("4160"),n("159b"),n("b4f1")),o={created:function(){window.scrollTo(0,0)},data:function(){return{md:String}},methods:{go_back:function(){window.history.back()},modifyTables:function(e){var t=document.getElementsByClassName("table");Array.prototype.forEach.call(t,(function(e){var t=e.parentNode,n=document.createElement("div");n.style.overflowX="auto",t.insertBefore(n,e),n.appendChild(e)})),this.md=e}},computed:{project:function(){var e=this;return a.find((function(t){return t.title===e.$route.params.detail}))}},metaInfo:function(){return{title:this.$route.params.detail}},track:function(){this.$ga.page("/project"+this.$route.params.detail)}},s=o,l=(n("b62c"),n("2877")),h=n("6544"),c=n.n(h),d=n("8336"),u=n("b0af"),m=(n("a9e3"),n("5530")),g=(n("63b7"),n("f665")),p=n("afdd"),f=n("9d26"),b=n("37c6"),y=n("604c"),w=y["a"].extend({name:"button-group",provide:function(){return{btnToggle:this}},computed:{classes:function(){return y["a"].options.computed.classes.call(this)}},methods:{genData:y["a"].options.methods.genData}}),v=n("80d2"),k=n("d9bd"),T=g["a"].extend({name:"v-carousel",props:{continuous:{type:Boolean,default:!0},cycle:Boolean,delimiterIcon:{type:String,default:"$delimiter"},height:{type:[Number,String],default:500},hideDelimiters:Boolean,hideDelimiterBackground:Boolean,interval:{type:[Number,String],default:6e3,validator:function(e){return e>0}},mandatory:{type:Boolean,default:!0},progress:Boolean,progressColor:String,showArrows:{type:Boolean,default:!0},verticalDelimiters:{type:String,default:void 0}},data:function(){return{internalHeight:this.height,slideTimeout:void 0}},computed:{classes:function(){return Object(m["a"])(Object(m["a"])({},g["a"].options.computed.classes.call(this)),{},{"v-carousel":!0,"v-carousel--hide-delimiter-background":this.hideDelimiterBackground,"v-carousel--vertical-delimiters":this.isVertical})},isDark:function(){return this.dark||!this.light},isVertical:function(){return null!=this.verticalDelimiters}},watch:{internalValue:"restartTimeout",interval:"restartTimeout",height:function(e,t){e!==t&&e&&(this.internalHeight=e)},cycle:function(e){e?this.restartTimeout():(clearTimeout(this.slideTimeout),this.slideTimeout=void 0)}},created:function(){this.$attrs.hasOwnProperty("hide-controls")&&Object(k["a"])("hide-controls",':show-arrows="false"',this)},mounted:function(){this.startTimeout()},methods:{genControlIcons:function(){return this.isVertical?null:g["a"].options.methods.genControlIcons.call(this)},genDelimiters:function(){return this.$createElement("div",{staticClass:"v-carousel__controls",style:{left:"left"===this.verticalDelimiters&&this.isVertical?0:"auto",right:"right"===this.verticalDelimiters?0:"auto"}},[this.genItems()])},genItems:function(){for(var e=this,t=this.items.length,n=[],r=0;r<t;r++){var i=this.$createElement(p["a"],{staticClass:"v-carousel__controls__item",attrs:{"aria-label":this.$vuetify.lang.t("$vuetify.carousel.ariaLabel.delimiter",r+1,t)},props:{icon:!0,small:!0,value:this.getValue(this.items[r],r)}},[this.$createElement(f["a"],{props:{size:18}},this.delimiterIcon)]);n.push(i)}return this.$createElement(w,{props:{value:this.internalValue,mandatory:this.mandatory},on:{change:function(t){e.internalValue=t}}},n)},genProgress:function(){return this.$createElement(b["a"],{staticClass:"v-carousel__progress",props:{color:this.progressColor,value:(this.internalIndex+1)/this.items.length*100}})},restartTimeout:function(){this.slideTimeout&&clearTimeout(this.slideTimeout),this.slideTimeout=void 0,window.requestAnimationFrame(this.startTimeout)},startTimeout:function(){this.cycle&&(this.slideTimeout=window.setTimeout(this.next,+this.interval>0?+this.interval:6e3))}},render:function(e){var t=g["a"].options.render.call(this,e);return t.data.style="height: ".concat(Object(v["f"])(this.height),";"),this.hideDelimiters||t.children.push(this.genDelimiters()),(this.progress||this.progressColor)&&t.children.push(this.genProgress()),t}}),x=n("1e6c"),D=n("adda"),I=n("58df"),S=n("1c87"),C=Object(I["a"])(x["a"],S["a"]),L=C.extend({name:"v-carousel-item",inheritAttrs:!1,methods:{genDefaultSlot:function(){return[this.$createElement(D["a"],{staticClass:"v-carousel__item",props:Object(m["a"])(Object(m["a"])({},this.$attrs),{},{height:this.windowGroup.internalHeight}),on:this.$listeners,scopedSlots:{placeholder:this.$scopedSlots.placeholder}},Object(v["l"])(this))]},genWindowItem:function(){var e=this.generateRouteLink(),t=e.tag,n=e.data;return n.staticClass="v-window-item",n.directives.push({name:"show",value:this.isActive}),this.$createElement(t,n,this.genDefaultSlot())}}}),j=n("a523"),_=n("132d"),A=n("da13"),P=n("5d23"),G=Object(l["a"])(s,r,i,!1,null,null,null);t["default"]=G.exports;c()(G,{VBtn:d["a"],VCard:u["a"],VCarousel:T,VCarouselItem:L,VContainer:j["a"],VIcon:_["a"],VImg:D["a"],VListItem:A["a"],VListItemContent:P["a"],VListItemTitle:P["c"]})},e76c:function(e,t,n){"use strict";n.r(t),t["default"]="## Motivation\r\nDespite it is a project for fulfilling the course requirement of [COMP5411 Advanced Computer Graphics](https://www.cse.ust.hk/pg/courses/#COMP5411), I took it as a challenge and implemented it within a week. The main initiatives are trying out `ray marching techniques` while learning `WebGL`.\r\nMost features are referencing [Fredrik's work](https://pdfs.semanticscholar.org/89e9/153a091889c584df034a953a0eff4de45ee9.pdf), including the height and density altering functions. The cloud shapes are carved out using 3d worley and perlin noises as suggested in [Guerrilla's slides](http://advances.realtimerendering.com/s2015/The%20Real-time%20Volumetric%20Cloudscapes%20of%20Horizon%20-%20Zero%20Dawn%20-%20ARTR.pdf) in SIGGRAPH 2015. Most work was done by me, except the noise generation and the sky shader from three.js demo.\r\n\r\n---\r\n\r\n## Development\r\n### Day 1\r\n![](/images/vc1.png) Since this is my first time of doing graphics on a browser, I wanted to test out the feasibility and get myself comfortable using such environment. So I tried to construct a semi-transparent 3D scalar field and ray marched through the volume. The result that I got is as expected, a semi-transparent cube, suspended in mid-air. The only early optimization that I can throw in is to terminate the ray marching when the cumulated opacity hits one. ~~Surely, no significant improvement at all.~~ A ghostly cube may not seem like much, but everything will come together when the volume is initialized with the right configurations.\r\n### Day 2\r\n|      Red       |     Green      |      Blue      |     Alpha      |\r\n| :------------: | :------------: | :------------: | :------------: |\r\n| ![](/images/vc2_r.png) | ![](/images/vc2_g.png) | ![](/images/vc2_b.png) | ![](/images/vc2_a.png) |\r\nNext, we need some information describing how our sky looks. And according to the method suggested in both the talk and paper, it can be achieved by encoding the cloud distribution in a RGBA texture called `Weather Map`. The red and green channels represent a `low-coverage` and `high-coverage` cloud distribution respectively. The blue channel represents the `maximum height` (altitude) the cloudscape can reach. The alpha channel represents the `maximum density` of the cloud distribution. With these information packed into a single texture, we could store the data in the GPU and query it efficiently during the shading stage. \r\nTo give more controls to the cloud distribution, a slider will be given for interpolating between the low and high coverage map. We then sample the interpolated value in each ray marching step to determine if a cloud is formed there. The maximum height map is basically a vertical mask introducing variations so that it's more visually natural. \r\nSo far the overall cloud distribution was modeled nicely. However, the resolution looks very bad since the 3D volume was essentially stretched across the sky. Therefore, we need to add some `noise` to further shape the cloudscape in detail.\r\n![](/images/vc2.png)\r\n\r\n### Day 3\r\n|      Red       |     Green      |      Blue      |     Alpha      |\r\n| :------------: | :------------: | :------------: | :------------: |\r\n| ![](/images/vc3_r.png) | ![](/images/vc3_g.png) | ![](/images/vc3_b.png) | ![](/images/vc3_a.png) |\r\nGenerating noise maps is time-consuming and definitely needed to be done offline, so I generated a raw 3D texture data during initialization and save it as a binary file. But for the sake of demonstration, the above illustrations are extracted from Fredrik's paper. It is a RGBA 128x128x128 3D texture consisting four noise maps. In the red channel, a `FBM` (Fractal Brownian Motion) noise is generated using `Perlin` noise alone. For the rest, are also FBM noises but a combination of `Perlin and Worley` noise with progressive frequencies. (**Note:** 3D texture sampling wasn't supported in WebGL1, that's the reason why WebGL2 is required for this to work)\r\nHere we are following the aesthetic of nature, the beauty of fractals, to add in layers of details to our fluffy clouds. Tiling the combined result of our noise maps and multiply with our weather map, the shape of a cloud begins to take place. For sanity check, the cloud distribution slider also worked as expected. It's satifying to watch the cloudscape spans across the sky.\r\n![](/images/vc3.png)\r\n\r\n### Day 4\r\nNow the basic geometry was done, it's time for some lighting work! To approximate the real physics going on inside a cloud, the light ray is needed to be traced back to the light source (sun) in each sample point to evaluate the amount of light that can be reached. However, iterating through each ray march steps and lighting steps could result in a squared complexity. I found that 2-3 lighting steps is enough for the minimum visual representation. More than that could result in severe performance drop (i.e. <30FPS on a 1080 display). \r\n`Beer's law` is used to calculate the attenuation value at each sample point. Here the density map in our weather map comes to play, as the law relies on the mediums properties. With the specified light color, the clouds are shaded correctly to the color we want.\r\n![](/images/vc4.png)\r\n\r\n### Day 5\r\nUntil now, I'm using a skybox as the background but that doesn't reflect the sun position and light direction. Luckily, three.js has an example code on a fully functional sky dome with dynamic lightings. Thus I shamelessly applied it in order to save my time. \r\nExtending from the previous work, the only adjustment needed to be done is to match our light color with the new sky color. Rather than extracting the sky color from the three.js example shader, I prefer hardcode the transition colors for artistic reason. \r\nEverything are now globally lit according to the sun position, yet something feels ... \\\"off\\\" in some sense. It's because the `silver-lining effect` is missing. See, normally clouds directly in-front of the sun would have a brighter outline as light shines behind it and reached our eyes. Therefore, the `Henyey-Greenstein's phase function` is augmented to our shading equation. It takes the in/out and isotropic scattering into accounts, and of course the light angle between the light direction and camera ray direction. \r\n![](/images/vc5.png) Now not only the cloud's outline looks better, but also you can easily tell where the sun is located even if it's occluded. Overall, the work is done here even though there're lots of optimization work could be done to run on lower specification devices. For example, the motion vector approach mentioned in the paper wasn't implemented here, and early termination in ray marching as to alleviate the average performance. \r\n\r\n### Extra\r\n![](/images/vc6.png)\r\nI've added a terrain model just for filling the emptiness. And you're able to navigate the enrionment using \\\"Minecraft-style\\\" controls. To tackle more on the banding effect caused by ray marching, a `blue noise` texture was applied to offset individual rays. \r\nMake sure to try out the [demo](https://tkchanat.github.io/webgl-volumetric-cloud/) if your browser supports WebGL2 (Google Chrome is recommended). Try to tweak on the parameters and observe the changes. :cloud::sunny::sunglasses:\r\n\r\n---\r\n\r\n## Tool Used\r\n- Programming Language: Javascript, WebGL2\r\n- Library Used: [three.js](https://threejs.org/), [tooloud.js](https://github.com/jackunion/tooloud), [dat.gui.js](https://github.com/dataarts/dat.gui), [stats.js](https://github.com/mrdoob/stats.js/)\r\n- Environment: Visual Studio Code, Google Chrome"},ec32:function(e,t,n){"use strict";n.r(t),t["default"]="## Description\r\nMy very first game project developed with early version of Unity3d. A first-person shooter game developed by me and my brother. Fun fact: This project was abandoned long ago due to the loss of source files. I really hope this project can go further. :(\r\n\r\n## About This Project\r\n- Programming Language: C#\r\n- Game Engine: Unity3D\r\n- Development Environment: MonoDevelop, Clip Studio Paint, Photoshop, Blender, 3DSMax, SketchUp\r\n- Team Size: 2\r\n- Role: Gameplay Programmer, 2D & 3D Artist\r\n\r\n## Responsibilities\r\n- Design and implement gameplay mechanics\r\n- Model and animate game assets\r\n- Conduct project flow\r\n\r\n## Useful Links\r\n- :link: [Demo Video](https://youtu.be/YSESoWVCvHU)\r\n- :link: [YouTube channel](https://www.youtube.com/user/1647andychan)\r\n- :link: [Animation Playlist](https://www.youtube.com/playlist?list=PLF7EEEDC74BA66BCB)"},ffd2:function(e,t,n){"use strict";n.r(t),t["default"]="## Motivation\r\nThis is a solo project for fulfilling the requirement of [COMP4431 Multimedia Computing](https://prog-crs.ust.hk/ugcourse/2019-20/COMP). In this project, we are told to extend our lab work and explore other image processing techniques. I have picked `Unsharp Masking` as my topic of investigation, as well as exploring some interesting application of using `Gausian blur`. For extra work, I have implemented the `Pencil Sketch` effect using similar technique.\r\n\r\n---\r\n\r\n## Description\r\n#### Unsharp Masking\r\nTo explain what is unsharp masking, first you shouldn't take its name literally. Because unsharp masking is actually a post-process operation to ***SHARPEN*** your image. If you want to understand further about its implementation, there are already lots of great articles on the internet. I could list out some that I've used for my own references:\r\n- :link:[Understanding Unsharp Mask](https://www.damiensymonds.net/tut_usm/) - Damien Symonds\r\n- :link:[Official Unsharp Mask Documentation](https://docs.gimp.org/2.6/en/plug-in-unsharp-mask.html) - GIMP\r\n- :link:[Sharpening: Unsharp Mask](https://www.cambridgeincolour.com/tutorials/unsharp-mask.htm) - Cambridge in Colour\r\nThe TL;DR version:\r\n1. Create a mask by subtracting a (Gausian) blurred copy from the original image.\r\n2. Add the mask on top of the (higher contrast) original image. \r\n3. You got the sharpened final image!\r\nNoted that the mask you created at step 1 might have some negative values, which is totally normal. Because there will be some overshooting/undershooting values along the edges. This will compensate the medium-ranged luminance pixels, such that darker gray will be darker and lighter gray will be lighter. Such separation of contrasted values then gives an illusion to the viewer that the image is \"sharpened\". But in reality, it is nothing but a regional contrast filter on the edges. \r\n\r\n|      Original      | Unsharp Masking  |\r\n| :----------------: | :--------------: |\r\n| ![](/images/lena_orig.png) | ![](/images/lena_um.png) |\r\n\r\nInitially I'd expect such filter could enhance the detail of a blurred image. But as I could observe from the following results, this doesn't do the work at all. Surely, it emphasizes the `contrast` along the edges. However, it can never reassemble the `lost features` on the edges. \r\n\r\n|   Blurred Image    |    Unsharp Masking    |\r\n| :----------------: | :-------------------: |\r\n| ![](/images/lena_blur.png) | ![](/images/lena_blur_um.png) |\r\n\r\n---\r\n\r\n#### Pencil Sketch\r\nSince I saw artists on the internet often use the combination of Gausian blur and layer blending to \"fake\" a pencil sketch effect. I would like to try replicating it for a little extra project work. \r\nHere's a simplified procedure:\r\n1. Prepare two grayscaled copies (A & B) of the original image. \r\n2. Invert the brightness of B.\r\n3. Apply Gaussian blur on either A or B.\r\n4. Layer B on top of A, and apply color dodge (and optionally combine them back to one image).\r\nAfter the above modifications, you now should be able to experiment with different `radius` value in the blurring operation, and get your desired pencil sketch effect. :pencil:\r\n\r\n|    Radius = 5    |    Radius = 20    |\r\n| :--------------: | :---------------: |\r\n| ![](/images/lena_ps.png) | ![](/images/lena_psh.png) |\r\n\r\n---\r\n\r\n## Tool Used\r\n- Programming Language: Javascript\r\n- Environment: VS Code"}}]);
//# sourceMappingURL=chunk-6c28d888.bdd48f90.js.map